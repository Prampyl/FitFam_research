

`H4_scripts.md`

-----

````
# üêç Analyse H4 : R√©silience Environnementale & Fid√©lit√©

Ce script a pour but de v√©rifier si les conditions difficiles (pollution en hiver, chaleur en √©t√©) agissent comme un filtre de fid√©lit√© pour les utilisateurs.

## 1. Chargement et Pr√©paration des Donn√©es
**Ce que fait ce bloc :**
Il importe les outils n√©cessaires (`pandas`) et charge vos fichiers bruts. Ensuite, il nettoie les donn√©es pour ne garder que les utilisateurs qui sont *vraiment* venus aux √©v√©nements (`checked_in == 1`) et associe chaque participation √† une date pr√©cise.

```python
import pandas as pd
import numpy as np

# 1. Chargement des fichiers JSON
# Ces fichiers doivent √™tre pr√©sents dans le m√™me dossier que le script
events = pd.read_json('events.json')
event_user = pd.read_json('event_user.json')
cancellation_reasons = pd.read_json('cancellation_reasons.json')
cancellation_reason_event = pd.read_json('cancellation_reason_event.json')

# 2. Conversion des dates au format temporel
# Indispensable pour pouvoir extraire les mois et calculer des dur√©es
events['start_time'] = pd.to_datetime(events['start_time'])
cancellation_reason_event['created_at'] = pd.to_datetime(cancellation_reason_event['created_at'])

# 3. Fusion des tables
# On ne garde que les participations effectives (checked_in == 1)
attended = event_user[event_user['checked_in'] == 1].copy()
# On ajoute les infos de l'√©v√©nement (date, id) √† chaque participation
df = attended.merge(events[['id', 'start_time']], left_on='event_id', right_on='id', how='left')

# Suppression des lignes sans date (bug de donn√©es potentiel)
df = df.dropna(subset=['start_time'])
````

## 2\. D√©finition des Saisons Extr√™mes

**Ce que fait ce bloc :**
Il cr√©e des rep√®res temporels. On extrait le mois de chaque √©v√©nement pour identifier s'il a eu lieu pendant une p√©riode "difficile" (Hiver ou √ât√© caniculaire).

  * **Hiver/Pollution :** D√©cembre, Janvier, F√©vrier
  * **√ât√©/Chaleur :** Juillet, Ao√ªt

<!-- end list -->

```python
# Extraction du mois et de l'ann√©e
df['month'] = df['start_time'].dt.month
df['year'] = df['start_time'].dt.year

# D√©finition des "Mois Extr√™mes"
# 1=Jan, 2=Fev, 7=Juil, 8=Ao√ªt, 12=D√©c
extreme_months = [1, 2, 7, 8, 12]

# On cr√©e une colonne 'is_extreme' qui vaut 1 si le mois est extr√™me, 0 sinon
df['is_extreme'] = df['month'].isin(extreme_months).astype(int)
```

## 3\. Analyse de la R√©tention (Dur√©e de vie)

**Ce que fait ce bloc :**
C'est le c≈ìur de l'analyse H4. Pour chaque utilisateur, on calcule :

1.  Sa dur√©e de vie (`tenure` : date de fin - date de d√©but).
2.  S'il a d√©j√† brav√© un mois extr√™me (`is_resilient`).
    Ensuite, on compare la dur√©e de vie moyenne des "guerriers" (r√©silients) vs ceux qui √©vitent les conditions difficiles.

<!-- end list -->

```python
# Agr√©gation des stats par utilisateur
user_stats = df.groupby('user_id').agg(
    first_event=('start_time', 'min'),       # Date premi√®re venue
    last_event=('start_time', 'max'),        # Date derni√®re venue
    total_attendance=('event_id', 'count'),  # Nombre total de venues
    extreme_attendance=('is_extreme', 'sum') # Nombre de venues en mois extr√™mes
).reset_index()

# Calcul de la "Tenure" en jours
user_stats['tenure_days'] = (user_stats['last_event'] - user_stats['first_event']).dt.days

# D√©finition du profil "R√©silient" : A particip√© au moins une fois en conditions extr√™mes
user_stats['is_resilient'] = user_stats['extreme_attendance'] > 0

# Comparaison finale
print("--- Comparaison de la R√©tention (R√©silients vs Non-R√©silients) ---")
print(user_stats.groupby('is_resilient')['tenure_days'].agg(['mean', 'median', 'count']))
```

## 4\. Analyse du "Churn" Saisonnier

**Ce que fait ce bloc :**
Il s√©pare les utilisateurs en deux groupes : les "Occasionnels" (Casual) et les "Fid√®les" (Core). On regarde ensuite mois par mois quel pourcentage de leur activit√© annuelle ils r√©alisent. Cela permet de voir si les occasionnels "disparaissent" l'hiver.

```python
# 1. Segmentation Casual vs Core
# On utilise la m√©diane (souvent 2 √©v√©nements) comme seuil
median_attendance = user_stats['total_attendance'].median()
user_stats['user_type'] = np.where(user_stats['total_attendance'] > median_attendance, 'Core', 'Casual')

# On remet cette info dans le tableau principal
df = df.merge(user_stats[['user_id', 'user_type']], on='user_id', how='left')

# 2. Calcul de la saisonnalit√©
# On compte les venues par mois pour chaque groupe
group_totals = df.groupby('user_type').size().reset_index(name='total_checkins')
monthly_seasonality = df.groupby(['user_type', 'month']).size().reset_index(name='checkins')

# Normalisation (pour avoir des pourcentages)
monthly_seasonality = monthly_seasonality.merge(group_totals, on='user_type')
monthly_seasonality['percentage'] = monthly_seasonality['checkins'] / monthly_seasonality['total_checkins']

print("\n--- Pourcentage d'activit√© par mois (Saisonnalit√©) ---")
# Affichage sous forme de tableau crois√© pour lecture facile
print(monthly_seasonality.pivot(index='month', columns='user_type', values='percentage'))
```

## 5\. Validation : Est-ce vraiment la pollution ?

**Ce que fait ce bloc :**
Pour √™tre s√ªr que l'hiver correspond bien √† la pollution (et non juste au froid), on regarde les *motifs d'annulation*. Si les annulations "AQI" (Qualit√© de l'air) explosent en hiver, l'hypoth√®se est valid√©e.

```python
# On extrait le mois de cr√©ation de l'annulation
cancellation_reason_event['month'] = cancellation_reason_event['created_at'].dt.month

# On compte les annulations par type et par mois
# ID 1 = Pollution (AQI > 150)
aqi_cancellations = cancellation_reason_event[cancellation_reason_event['cancellation_reason_id'] == 1].groupby('month').size()

# ID 2 = Mauvaise M√©t√©o (Pluie/Typhon)
weather_cancellations = cancellation_reason_event[cancellation_reason_event['cancellation_reason_id'] == 2].groupby('month').size()

print("\n--- Annulations cause 'Pollution' par mois ---")
print(aqi_cancellations)

print("\n--- Annulations cause 'M√©t√©o' par mois ---")
print(weather_cancellations)
```

```
```